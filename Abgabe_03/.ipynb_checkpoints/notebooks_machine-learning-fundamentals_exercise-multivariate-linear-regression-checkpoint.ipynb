{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# ML-Fundamentals - Lineare Regression - Exercise: Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#Introduction)\n",
    "* [Requirements](#Requirements) \n",
    "  * [Knowledge](#Knowledge) \n",
    "  * [Modules](#Python-Modules)\n",
    "* [Exercises - Multivariate Linear Regression](#Exercises---Multivariate-Linear-Regression)\n",
    "  * [Create Features](#Create-Features)\n",
    "  * [Linear Hypothesis](#Linear-Hypothesis)\n",
    "  * [Generate Target Values](#Generate-Target-Values)\n",
    "  * [Plot The Data](#Plot-The-Data)\n",
    "  * [Cost Function](#Cost-Function)\n",
    "  * [Gradient Descent](#Gradient-Descent)\n",
    "  * [Training and Evaluation](#Training-and-Evaluation)\n",
    "  * [Feature Scaling](#Feature-Scaling)\n",
    "* [Summary and Outlook](#Summary-and-Outlook)\n",
    "* [Literature](#Literature) \n",
    "* [Licenses](#Licenses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this exercise you will implement the _multivariate linear regression_, a model with two or more predictors and one response variable (opposed to one predictor using univariate linear regression). The whole exercise consists of the following steps:\n",
    "\n",
    "1. Generate values for two predictors/features $(x_1, x_2)$\n",
    "2. Implement a linear function as hypothesis (model) \n",
    "3. Generate values for the response (Y / target values)\n",
    "4. Plot the $((x_1, x_2), y)$ values in a 3D plot.\n",
    "5. Write a function to quantify your model (cost function)\n",
    "6. Implement the gradient descent algorithm to train your model (optimizer) \n",
    "7. Visualize your training process and results\n",
    "8. Apply feature scaling (pen & paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Requirements\n",
    "### Knowledge\n",
    "\n",
    "You should have a basic knowledge of:\n",
    "- Univariate linear regression\n",
    "- Multivariate linear regression\n",
    "- Squared error\n",
    "- Gradient descent\n",
    "- numpy\n",
    "- matplotlib\n",
    "\n",
    "Suitable sources for acquiring this knowledge are:\n",
    "- [Multivariate Linear Regression Notebook](http://christianherta.de/lehre/dataScience/machineLearning/basics/multivariate_linear_regression.php) by Christian Herta and his [lecture slides](http://christianherta.de/lehre/dataScience/machineLearning/multivariateLinearRegression.pdf) (German)\n",
    "- Chapter 2 of the open classroom [Machine Learning](http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning) by Andrew Ng\n",
    "- Chapter 5.1 of [Deep Learning](http://www.deeplearningbook.org/contents/ml.html) by Ian Goodfellow \n",
    "- Some parts of chapter 1 and 3 of [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book) by Christopher M. Bishop\n",
    "- [numpy quickstart](https://docs.scipy.org/doc/numpy-1.15.1/user/quickstart.html)\n",
    "- [Matplotlib tutorials](https://matplotlib.org/tutorials/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Python Modules\n",
    "\n",
    "By [deep.TEACHING](https://www.deep-teaching.org/) convention, all python modules needed to run the notebook are loaded centrally at the beginning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# External Modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise - Multivariate Linear Regression\n",
    "\n",
    "We will only use two features in this notebook, so we are still able to plot them together with the target in a 3D plot. But your implementation should also be capable of handling more (except the plots). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Create Features\n",
    "\n",
    "First we will create some features. The features should be in a 2D numpy array, the rows separating the different feature vectors, the columns containing the features. Each feature should be uniformly distributed in a specifiable range.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Implement the function to generate a feature matrix (numpy array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def create_feature_matrix(sample_size, n_features, x_min, x_max):\n",
    "    '''creates random feature vectors based on a lienar function in a given interval\n",
    "    \n",
    "    Args:\n",
    "        sample_size: number feature vectors\n",
    "        n_features: number of features for each vector\n",
    "        x_min: lower bound value ranges\n",
    "        x_max: upper bound value ranges\n",
    "    \n",
    "    Returns:\n",
    "        x: 2D array containing feature vecotrs with shape (sample_size, n_features)\n",
    "    '''\n",
    "    x = np.ndarray((sample_size,n_features))\n",
    "    for i in range(n_features):\n",
    "        feature = np.random.uniform(x_min[i], x_max[i], size=sample_size)\n",
    "        x[:,i] = feature\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100\n",
    "n_features = 2\n",
    "x_min = [1.5,-0.5]\n",
    "x_max = [11.,5.0]\n",
    "\n",
    "X = create_feature_matrix(sample_size, n_features, x_min, x_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X[:,0]) == sample_size\n",
    "assert len(X[0,:]) == n_features\n",
    "for i in range(n_features):\n",
    "    assert np.max(X[:,i]) <= x_max[i]\n",
    "    assert np.min(X[:,i]) >= x_min[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Linear Hypothesis\n",
    "\n",
    "\n",
    "A short recap, a hypothesis $h_\\theta(x)$ is a certain function that we believe is similar to a target function that we like to model. A hypothesis $h_\\theta(x)$ is a function of $x$ with fixed parameters $\\theta$. \n",
    "\n",
    "Here we have $n$ features $x = [x_1, \\ldots, x_n ]$ and $n+1$ $\\theta$s:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_{0} + \\theta_{1} x_1 + \\ldots \\theta_n x_n \n",
    "$$\n",
    "\n",
    "adding an extra element to $x$ for convenience, this could also be rewritten as:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_{0} x_0 + \\theta_{1} x_1 + \\ldots \\theta_n x_n \n",
    "$$\n",
    "\n",
    "with $x_0 = 1$ for all feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or treating $x$ and $\\theta$ as vectors:\n",
    "\n",
    "$$\n",
    "h(\\vec x) = \\vec x'^T \\vec \\theta\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$\n",
    "\\vec x = \\begin{pmatrix} \n",
    "x_1 & x_2 & \\ldots & x_n \\\\\n",
    "\\end{pmatrix}^T\n",
    "\\text{   and   }\n",
    "\\vec x' = \\begin{pmatrix} \n",
    "1 & x_1 & x_2 & \\ldots & x_n \\\\\n",
    "\\end{pmatrix}^T\n",
    "$$\n",
    "\n",
    "Or for the whole data set at once: The rows in $\\vec X$ separate the different feature vectors, the columns contain the features. \n",
    "\n",
    "$$\n",
    "h(\\vec x) = X' \\vec \\theta\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$\n",
    "X = \\begin{pmatrix} \n",
    "x_1^1 & \\ldots & x_n^1 \\\\\n",
    "x_1^2 & \\ldots & x_n^2 \\\\\n",
    "\\vdots &\\vdots &\\vdots \\\\\n",
    "x_1^m & \\ldots & x_n^m \\\\\n",
    "\\end{pmatrix}\n",
    "\\text{   and   }\n",
    "X' = \\begin{pmatrix} \n",
    "1 & x_1^1 & \\ldots & x_n^1 \\\\\n",
    "1 & x_1^2 & \\ldots & x_n^2 \\\\\n",
    "\\vdots &\\vdots &\\vdots &\\vdots \\\\\n",
    "1 & x_1^m & \\ldots & x_n^m \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "\n",
    "Implement hypothesis $h_\\theta(x)$ in the method `linear_hypothesis` and return it as a function. Implement it the computationally efficient (**pythonic**) way by not using any loops and handling all data at once (use $X$ respectively $X'$).\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Of course you are free to implement as many helper functions as you like, e.g. for transforming $X$ to $X'$, though you do not have to. Up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def linear_hypothesis(thetas):\n",
    "    ''' Combines given list argument in a linear equation and returns it as a function\n",
    "    \n",
    "    Args:\n",
    "        thetas: list of coefficients\n",
    "        \n",
    "    Returns:\n",
    "        lambda that models a linear function based on thetas and x\n",
    "    ''' \n",
    "    def get_X_(X):\n",
    "        ones = np.ones((X.shape[0],1))\n",
    "        X_ = np.c_[ones, X]\n",
    "        return X_\n",
    "        \n",
    "    return lambda X: np.dot(get_X_(X), thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(linear_hypothesis([.1,.2,.3])(X)) == sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Target Values\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Use your implemented `linear_hypothesis` inside the next function to generate some target values $Y$. Additionally add some Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def generate_targets(X, theta, sigma):\n",
    "    ''' Combines given arguments in a linear equation with X, \n",
    "    adds some Gaussian noise and returns the result\n",
    "    \n",
    "    Args:\n",
    "        X: 2D numpy feature matrix\n",
    "        theta: list of coefficients\n",
    "        sigma: standard deviation of the gaussian noise\n",
    "        \n",
    "    Returns:\n",
    "        target values for X\n",
    "    '''\n",
    "    noise = np.random.normal(0,sigma,(X.shape[0],1))\n",
    "    X_noise = X + noise # add noise so first column keeps X0 = 1 values\n",
    "    target_values = linear_hypothesis(theta)(X_noise)\n",
    "    return target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = (2., 3., -4.)\n",
    "sigma = 3.\n",
    "\n",
    "# feature scaling\n",
    "# X[:,0] = (X[:,0] - np.mean(X[:,0])) / np.std(X[:,0])\n",
    "# X[:,1] = (X[:,1] - np.mean(X[:,1])) / np.std(X[:,1])\n",
    "\n",
    "y = generate_targets(X, theta, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(y) == sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot The Data\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Plot the data $D = \\{(x^{(1)}_1,x^{(1)}_2,y^{(1)}), \\ldots, (x^{(n)}_1,x^{(n)}_2,y^{(n)})\\}$ in a 3D scatter plot. The plot should look like the following:\n",
    "\n",
    "<img src=\"https://gitlab.com/deep.TEACHING/educational-materials/raw/dev/media/klaus/exercise-multivariate-linear-regression-scatter.png\" width=\"512\" alt=\"internet connection needed\">\n",
    "\n",
    "**Sidenote:**\n",
    "\n",
    "The command `%matplotlib notebook` (instead of `%matplotlib inline`) creates an interactive (e.g. rotatable) plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "def plot_data_scatter(features, targets):\n",
    "    \"\"\" Plots the features and the targets in a 3D scatter plot\n",
    "    \n",
    "    Args:\n",
    "        features: 2D numpy-array features\n",
    "        targets: ltargets\n",
    "    \"\"\"\n",
    "    x1 = features[:,0]\n",
    "    x2 = features[:,1]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(x1, x2, y, c='r', marker='o')\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_zlabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cost Function\n",
    "A cost function $J$ depends on the given training data $D$ and hypothesis $h_\\theta(x)$. In the context of the linear regression, the cost function measures how wrong a model is regarding its ability to estimate the relationship between $x$ and $y$ for specific $\\theta$ values. Later we will treat this as an optimization problem and try to minimize the cost function $J_D(\\theta)$ to find optimal $\\theta$ values for our hypothesis $h_\\theta(x)$. The cost function we use in this exercise is the [Mean-Squared-Error](https://en.wikipedia.org/wiki/Mean_squared_error) cost function:\n",
    "\n",
    "\\begin{equation}\n",
    "    J_D(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}{(h_\\theta(x_i)-y_i)^2}\n",
    "\\end{equation}\n",
    "\n",
    "Implement the cost function $J_D(\\theta)$ in the method `mse_cost_function`. The method should return a function that takes the values of $\\theta$ as an argument.\n",
    "\n",
    "As a sidenote, the terms \"loss function\" or \"error function\" are often used interchangeably in the field of Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def mse_cost_function(x, y):\n",
    "    ''' Implements MSE cost function as a function J(theta) on given traning data \n",
    "    \n",
    "    Args:\n",
    "        x: vector of x values \n",
    "        y: vector of ground truth values y \n",
    "        \n",
    "    Returns:\n",
    "        lambda J(theta) that models the cost function\n",
    "    '''\n",
    "    return lambda theta: np.mean((linear_hypothesis(theta)(x) - y)**2)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the cell in which you generate the target values and note the theta values, which were used for it (If you haven't edited the default values, it should be `[2, 3, -4]`)\n",
    "\n",
    "**Optional:**\n",
    "\n",
    "Try a few different values for theta to pass to the cost function - Which thetas result in a low error and which produce a great error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = mse_cost_function(X, y)\n",
    "print(J(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Gradient Descent\n",
    "\n",
    "A short recap, the gradient descent algorithm is a first-order iterative optimization for finding a minimum of a function. From the current position in a (cost) function, the algorithm steps proportional to the negative of the gradient and repeats this until it reaches a local or global minimum and determines. Stepping proportional means that it does not go entirely in the direction of the negative gradient, but scaled by a fixed value $\\alpha$ also called the learning rate. Implementing the following formalized update rule is the core of the optimization process:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\theta_{j_{new}} \\leftarrow \\theta_{j_{old}} - \\alpha * \\frac{\\delta}{\\delta\\theta_{j_{old}}} J(\\theta_{old})\n",
    "\\end{equation}\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Implement the function to update all theta values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def update_theta(x, y, theta, learning_rate):\n",
    "    ''' Updates learnable parameters theta \n",
    "    \n",
    "    The update is done by calculating the partial derivities of \n",
    "    the cost function including the linear hypothesis. The \n",
    "    gradients scaled by a scalar are subtracted from the given \n",
    "    theta values.\n",
    "    \n",
    "    Args:\n",
    "        x: 2D numpy array of x values\n",
    "        y: array of y values corresponding to x\n",
    "        theta: current theta values\n",
    "        learning_rate: value to scale the negative gradient \n",
    "        \n",
    "    Returns:\n",
    "        theta: Updated theta vector\n",
    "    '''\n",
    "    partialDerivative = np.mean(x.T @ (linear_hypothesis(theta)(x) - y))\n",
    "    theta = theta - learning_rate * partialDerivative\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Using the `update_theta` method, you can now implement the gradient descent algorithm. Iterate over the update rule to find the values for $\\theta$ that minimize our cost function $J_D(\\theta)$. This process is often called training of a machine learning model. \n",
    "\n",
    "**Task:**\n",
    "- Implement the function for the gradient descent.\n",
    "- Create a history of all theta and cost values and return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(learning_rate, theta, iterations, x, y):\n",
    "    ''' Minimize theta values of a linear model based on MSE cost function\n",
    "    \n",
    "    Args:\n",
    "        learning_rate: scalar, scales the negative gradient \n",
    "        theta: initial theta values\n",
    "        x: vector, x values from the data set\n",
    "        y: vector, y values from the data set\n",
    "        iterations: scalar, number of theta updates\n",
    "        \n",
    "    Returns:\n",
    "        history_cost: cost after each iteration\n",
    "        history_theta: Updated theta values after each iteration\n",
    "    '''\n",
    "    history_cost = []\n",
    "    history_theta = []\n",
    "    j = mse_cost_function(x, y)\n",
    " \n",
    "    for i in range(iterations):\n",
    "        history_cost.append(j(theta))\n",
    "        history_theta.append(theta)\n",
    "        theta = update_theta(x, y, theta, learning_rate)\n",
    "    \n",
    "    return history_cost, history_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Choose an appropriate learning rate, number of iterations and initial theta values and start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation:\n",
    "\n",
    "alpha = 0.0001 # assign an appropriate value\n",
    "nb_iterations = 100 # assign an appropriate value\n",
    "start_values_theta = [7,7,7] # assign appropriate values\n",
    "history_cost, history_theta = gradient_descent(alpha, start_values_theta, nb_iterations, X, y)\n",
    "print(history_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training has finished we can visualize our results.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Plot the costs over the iterations. If you have used `fig = plt.figure()` and `ax = fig.add_subplot(111)` in the last plot, use it again here, else the plot will be added to the last plot instead of a new one.\n",
    "\n",
    "Your plot should look similar to this one:\n",
    "\n",
    "<img src=\"https://gitlab.com/deep.TEACHING/educational-materials/raw/dev/media/klaus/exercise-multivariate-linear-regression-costs.png\" width=\"512\" alt=\"internet connection needed\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_progress(costs):\n",
    "    \"\"\" Plots the costs over the iterations\n",
    "    \n",
    "    Args:\n",
    "        costs: history of costs\n",
    "    \"\"\"\n",
    "    iterations = np.arange(nb_iterations)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(iterations,costs, label=\"Fortschritt\")\n",
    "    ax.set_xlabel('Iterationen')\n",
    "    ax.set_ylabel('Kosten')\n",
    "    ax.set_title(\"Cost Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_progress(history_cost)\n",
    "print(\"costs before the training:\\t \", history_cost[0])\n",
    "print(\"costs after the training:\\t \", history_cost[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "\n",
    "Finally plot the decision hyperplane (just a plain plane here though) together with the data in a 3D plot.\n",
    "\n",
    "Your plot should look similar to this one:\n",
    "\n",
    "<img src=\"https://gitlab.com/deep.TEACHING/educational-materials/raw/dev/media/klaus/exercise-multivariate-linear-regression-scatter_and_boundary.png\" width=\"512\" alt=\"internet connection needed\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation_plt(x, y, final_theta):\n",
    "    ''' Plots the data x, y together with the final model\n",
    "    \n",
    "    Args:\n",
    "        cost_hist: vector, history of all cost values from a opitmization\n",
    "        theta_0: scalar, model parameter for boundary\n",
    "        theta_1: scalar, model parameter for boundary\n",
    "        x: vector, x values from the data set\n",
    "        y: vector, y values from the data set\n",
    "    '''\n",
    "    x1 = x[:,0]\n",
    "    x2 = x[:,1]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(x1, x2, y, c='r', marker='o')\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x1.min(), x1.max(), 20), \n",
    "                       np.linspace(x2.min(), x2.max(), 20))\n",
    "    # plot the hyperplane by evaluating the parameters on the grid\n",
    "    z = final_theta[0] + final_theta[1] * xx1 + final_theta[2] * xx2\n",
    "    \n",
    "    ax.plot_surface(xx1, xx2, z, rstride=1, cstride=1, cmap='jet')\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_zlabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_plt(X, y, history_theta[-1])\n",
    "print(\"thetas before the training:\\t\", history_theta[0])\n",
    "print(\"thetas after the training:\\t\", history_theta[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Now suppose the following features $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.0001, 2000],\n",
    "       [0.0002, 1800],\n",
    "       [0.0003, 1600]], dtype=np.float32)\n",
    "\n",
    "sample_size = len(X[:,0])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Optional:*\n",
    "\n",
    "You can even execute the cell above and start running your notebook again from top (all **except** executing the cell to generate your features, which would overwrite these new features).\n",
    "\n",
    "When you start training you should notice that your costs do not decrease, maybe even increase, if you have not adjusted your learning rate (training might also throw an overflow warning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "\n",
    "This task can be done via **pen & paper** or by inserting some code below. Either way, you should be able to solve both tasks below on paper only using a calculator.\n",
    "\n",
    "1. Apply feature scaling onto `X` using the *mean* and the *standard deviation*. What values do the scaled features have?\n",
    "\n",
    "2. After the training with scaled features your new $\\theta'$ values will be very high, something like: $\\theta'=[-7197,  326, -326]$ (you can try it but you do not have to). Suppose $\\theta'=[-7197,  326, -326]$. What are the corresponding $\\theta$ values for the unscaled data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.\n",
    "Xj = np.zeros(X.shape)\n",
    "Xj[:,0] = (X[:,0] - np.mean(X[:,0])) / np.std(X[:,0])\n",
    "Xj[:,1] = (X[:,1] - np.mean(X[:,1])) / np.std(X[:,1])\n",
    "print(Xj)\n",
    "print('-------------------------------')\n",
    "print('mean1: ' + str(np.mean(Xj[:,0])))\n",
    "print('mean2: ' + str(np.mean(Xj[:,1])))\n",
    "print('std1: ' + str(np.std(Xj[:,0])))\n",
    "print('std2: ' + str(np.std(Xj[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.\n",
    "thetas = [-7197, 326, -326]\n",
    "theta0 = thetas[0] - thetas[1] * np.mean(X[:,0])/ np.var(X[:,0])\n",
    "theta1 = thetas[1] / np.var(X[:,0])\n",
    "theta2 = thetas[2] / np.var(X[:,1])\n",
    "print(theta0,theta1,theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary and Outlook\n",
    "\n",
    "During this exercise, the linear regression was extended to multidimensional feature space and feature scaling was practiced. You should be able to answer the following questions:\n",
    "- How does the implementation of the multivariate regression differ from the univariate one?\n",
    "- Why do we apply feature scaling?\n",
    "- Why does feature scaling help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Licenses\n",
    "\n",
    "### Notebook License (CC-BY-SA 4.0)\n",
    "\n",
    "*The following license applies to the complete notebook, including code cells. It does however not apply to any referenced external media (e.g., images).*\n",
    "\n",
    "Exercise: Multivariate Linear Regression <br/>\n",
    "by Christian Herta, Klaus Strohmenger<br/>\n",
    "is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).<br/>\n",
    "Based on a work at https://gitlab.com/deep.TEACHING.\n",
    "\n",
    "\n",
    "### Code License (MIT)\n",
    "\n",
    "*The following license only applies to code cells of the notebook.*\n",
    "\n",
    "Copyright 2018 Christian Herta, Klaus Strohmenger\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
